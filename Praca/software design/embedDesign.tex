% !TeX spellcheck = en_EN-English

\section{Embedding}
\label{embedDesign}

For embedding, we used a couple of additional libraries since we utilized several algorithms and pre-trained models. More specifically, the libraries and specific algorithms and models we used are the following:

\begin{itemize}
	\item \texttt{Scikit-learn 1.2.2} – simple and efficient tools for predictive data analysis \cite{scikitlearn}. We specifically used the function to compute PCA in order to decrease the dimensionality of medical procedure embeddings, and the K-means clustering function to check whether the embedding has the desired property.
	
	\item \texttt{NLTK 3.8.1} – this abbreviation stands for Natural Language Toolkit, it's a library for building Python programs to work with human language data \cite{ntlk}. In our case, we used the tokenizer function to split descriptions of medical procedures into tokens, in our case, words.
	
	\item \texttt{Simplemma 1.1.2} – provides a simple and multilingual approach to finding base forms or lemmata \cite{simplemma}. We used it to lemmatize our tokenized text since the lemmatizer provided by this library includes Slovak and Czech languages.
	
	\item \texttt{SentenceTransformer 2.2.2} – a go-to Python module for accessing, using, and training state-of-the-art text and image embedding models \cite{sentence_transformer}, which allowed us to easily load the LaBSE model from Hugging Face.
	
	\item \texttt{Gensim 4.3.3} – a library for topic modelling, document indexing, and similarity retrieval with large corpora \cite{gensim}, which we used to load the Word2vec model trained specifically for the Slovak language.
\end{itemize}


