\newcommand{\noopsort}[1]{} \newcommand{\printfirst}[2]{#1}
  \newcommand{\singleletter}[1]{#1} \newcommand{\switchargs}[2]{#2#1}
\begin{thebibliography}{10}

\bibitem{atc_who}
{A}natomical {T}herapeutic {C}hemical ({A}{T}{C}) {C}lassification --- who.int.
\newblock \url{https://www.who.int/tools/atc-ddd-toolkit/atc-classification}.
\newblock [Accessed 25-09-2024].

\bibitem{w2v}
{G}it{H}ub - essential-data/word2vec-sk: {V}ector representations of {S}lovak
  words trained using word2vec --- github.com.
\newblock \url{https://github.com/essential-data/word2vec-sk}.
\newblock [Accessed 04-03-2025].

\bibitem{word2vec}
{G}it{H}ub - essential-data/word2vec-sk: {V}ector representations of {S}lovak
  words trained using word2vec --- github.com.
\newblock \url{https://github.com/essential-data/word2vec-sk}.
\newblock [Accessed 19-10-2024].

\bibitem{labse_kaggle}
{G}oogle | {L}a{B}{S}{E} | {K}aggle --- kaggle.com.
\newblock
  \url{https://www.kaggle.com/models/google/labse/tensorFlow2/labse/1?tfhub-redirect=true}.
\newblock [Accessed 18-10-2024].

\bibitem{pytorchLSTM}
{L}{S}{T}{M} --- {P}y{T}orch 2.6 documentation --- pytorch.org.
\newblock \url{https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html}.
\newblock [Accessed 23-03-2025].

\bibitem{ncziMKCH}
Medzinárodná klasifikácia chorôb - {M}{K}{C}{H}-10 --- nczisk.sk.
\newblock
  \url{https://www.nczisk.sk/Standardy-v-zdravotnictve/Pages/Medzinarodna-klasifikacia-chorob-MKCH-10.aspx}.
\newblock [Accessed 16-09-2024].

\bibitem{labse_hug}
sentence-transformers/{L}a{B}{S}{E} · {H}ugging {F}ace --- huggingface.co.
\newblock \url{https://huggingface.co/sentence-transformers/LaBSE}.
\newblock [Accessed 19-10-2024].

\bibitem{elman_img}
Sivanand Achanta, Rambabu Banoth, Ayushi Pandey, Anandaswarup Vadapalli, and
  Suryakanth~V Gangashetty.
\newblock Contextual representation using recurrent neural network hidden state
  for statistical parametric speech synthesis.
\newblock In {\em SSW}, pages 172--177, 2016.

\bibitem{simplemma}
Adrien Barbaresi.
\newblock Simplemma, January 2023.

\bibitem{num_of_vis}
Andreas Berzel, Gillian~Z Heller, and Walter Zucchini.
\newblock Estimating the number of visits to the doctor.
\newblock {\em Australian \& New Zealand Journal of Statistics},
  48(2):213--224, 2006.

\bibitem{ntlk}
Steven Bird, Ewan Klein, and Edward Loper.
\newblock {\em Natural language processing with Python: analyzing text with the
  natural language toolkit}.
\newblock " O'Reilly Media, Inc.", 2009.

\bibitem{caballer2019predicting}
Vicent Caballer-Tarazona, Natividad Guadalajara-Olmeda, and David
  Vivas-Consuelo.
\newblock Predicting healthcare expenditure by multimorbidity groups.
\newblock {\em Health Policy}, 123(4):427--434, 2019.

\bibitem{cdcICD10CM}
CDC.
\newblock {I}{C}{D}-10-{C}{M} --- cdc.gov.
\newblock \url{https://www.cdc.gov/nchs/icd/icd-10-cm/index.html}.
\newblock [Accessed 16-09-2024].

\bibitem{chechulin2014predicting}
Yuriy Chechulin, Amir Nazerian, Saad Rais, and Kamil Malikov.
\newblock Predicting patients with high risk of becoming high-cost healthcare
  users in ontario (canada).
\newblock {\em Healthcare Policy}, 9(3):68, 2014.

\bibitem{choi2018mime}
Edward Choi, Cao Xiao, Walter Stewart, and Jimeng Sun.
\newblock Mime: Multilevel medical embedding of electronic health records for
  predictive healthcare.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{bert_pretr_3}
Alexis Conneau and Guillaume Lample.
\newblock Cross-lingual language model pretraining.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{bert_pretr_1}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In Jill Burstein, Christy Doran, and Thamar Solorio, editors, {\em
  Proceedings of the 2019 Conference of the North {A}merican Chapter of the
  Association for Computational Linguistics: Human Language Technologies,
  Volume 1 (Long and Short Papers)}, pages 4171--4186, Minneapolis, Minnesota,
  June 2019. Association for Computational Linguistics.

\bibitem{bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186, 2019.

\bibitem{elman}
Jeffrey~L Elman.
\newblock Finding structure in time.
\newblock {\em Cognitive science}, 14(2):179--211, 1990.

\bibitem{labse_paper}
Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang.
\newblock Language-agnostic bert sentence embedding.
\newblock In {\em Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 878--891, 2022.

\bibitem{numpy}
Charles~R. Harris, K.~Jarrod Millman, St{\'{e}}fan~J. van~der Walt, Ralf
  Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor,
  Sebastian Berg, Nathaniel~J. Smith, Robert Kern, Matti Picus, Stephan Hoyer,
  Marten~H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime~Fern{\'{a}}ndez
  del R{\'{i}}o, Mark Wiebe, Pearu Peterson, Pierre G{\'{e}}rard-Marchant,
  Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph
  Gohlke, and Travis~E. Oliphant.
\newblock Array programming with {NumPy}.
\newblock {\em Nature}, 585(7825):357--362, September 2020.

\bibitem{MLParch}
Miao Jin, Qinzhuo Liao, Shirish Patil, Abdulazeez Abdulraheem, Dhafer
  Al-Shehri, and Guenther Glatz.
\newblock Hyperparameter tuning of artificial neural networks for well
  production estimation considering the uncertainty in initialized parameters.
\newblock {\em ACS omega}, 7(28):24145--24156, 2022.

\bibitem{adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{cbow}
Waldemar Lopez.
\newblock {\em VECTOR REPRESENTATION OF INTERNET DOMAIN NAMES USING WORD
  EMBEDDING TECHNIQUES}.
\newblock PhD thesis, 11 2019.

\bibitem{lorenzi2017predictive}
Elizabeth~C Lorenzi, Stephanie~L Brown, Zhifei Sun, and Katherine Heller.
\newblock Predictive hierarchical clustering: Learning clusters of cpt codes
  for improving surgical outcomes.
\newblock In {\em Machine Learning for Healthcare Conference}, pages 231--242.
  PMLR, 2017.

\bibitem{w2vOrigPaper}
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
\newblock Efficient estimation of word representations in vector space, 2013.

\bibitem{skipgram}
Nada Mimouni, Jean-Claude Moissinac, and Anh~Tuan Vu.
\newblock Domain specific knowledge graph embedding for analogical link
  discovery.
\newblock 06 2020.

\bibitem{miotto2016deep}
Riccardo Miotto, Li~Li, and Joel~T Dudley.
\newblock Deep learning to predict patient future diseases from the electronic
  health records.
\newblock In {\em Advances in Information Retrieval: 38th European Conference
  on IR Research, ECIR 2016, Padua, Italy, March 20--23, 2016. Proceedings 38},
  pages 768--774. Springer, 2016.

\bibitem{morid2018supervised}
Mohammad~Amin Morid, Kensaku Kawamoto, Travis Ault, Josette Dorius, and Samir
  Abdelrahman.
\newblock Supervised learning methods for predicting healthcare costs:
  systematic literature review and empirical evaluation.
\newblock In {\em AMIA annual symposium proceedings}, volume 2017, page 1312,
  2018.

\bibitem{morid2019healthcare}
Mohammad~Amin Morid, Olivia R~Liu Sheng, Kensaku Kawamoto, Travis Ault, Josette
  Dorius, and Samir Abdelrahman.
\newblock Healthcare cost prediction: Leveraging fine-grain temporal patterns.
\newblock {\em Journal of biomedical informatics}, 91:103113, 2019.

\bibitem{chatGPT}
Online.
\newblock In: Chatgpt vezia 4.
\newblock {\em Available at: OpenAI, URL}, Task:.

\bibitem{pandas}
The pandas~development team.
\newblock pandas-dev/pandas: Pandas, February 2020.

\bibitem{pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan
  Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith
  Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library,
  2019.

\bibitem{scikitlearn}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel,
  M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos,
  D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock {\em Journal of Machine Learning Research}, 12:2825--2830, 2011.

\bibitem{gensim}
Radim Rehurek and Petr Sojka.
\newblock Gensim--python framework for vector space modelling.
\newblock {\em NLP Centre, Faculty of Informatics, Masaryk University, Brno,
  Czech Republic}, 3(2), 2011.

\bibitem{sentence_transformer}
Nils Reimers and Iryna Gurevych.
\newblock Sentence-bert: Sentence embeddings using siamese bert-networks.
\newblock In {\em Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing}. Association for Computational Linguistics, 11
  2019.

\bibitem{bert_pretr_2}
Yi~Sun, Yu~Zheng, Chao Hao, and Hangping Qiu.
\newblock Nsp-bert: A prompt-based few-shot learner through an original
  pre-training task--next sentence prediction.
\newblock {\em arXiv preprint arXiv:2109.03564}, 2021.

\bibitem{attentionAllYouNeed}
A~Vaswani.
\newblock Attention is all you need.
\newblock {\em Advances in Neural Information Processing Systems}, 2017.

\end{thebibliography}
