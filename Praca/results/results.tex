% !TeX spellcheck = en_EN-English

\chapter{Results} \label{chap:results}

In this chapter, we will report and discuss the training and testing of the models we identified as the best performing on the training subset of data in Sec. \ref{costPredRes} and Sec. \ref{recordPredRes}, now trained on the complete training dataset.
\\

\section{Training of final models}
\label{modelTrain}

Both MLP and Decoder-only Transformer models were trained on a dataset consisting of 156 020 patients (approximately 90\% of all patients in our complete dataset). Each model was trained for 10 epochs.
\\

For the MLP model, we settled on a batch size of 512 and an initial learning rate of 0.0004. After the final epoch, the model achieved a training loss of 0.0303 and a training accuracy of 80.2\%, indicating a slight improvement over the model trained on the subset of data. However, the improvement is so minor that it suggests either reaching the upper limit of extractable information from the data or an unidentified flaw in our approach.
\\

For the Decoder-only Transformer model, we used a lookback value of 20, meaning the model always processed the last 20 patient records to predict the 21st record. Due to the increased input size, we used a significantly smaller batch size of 64 compared to the MLP model. The initial learning rate was set to 0.0025. Training with these parameters resulted in a final training loss of 0.4005, which is significantly lower than the 0.5200 training loss of the model trained on the subset.

\section{Validation of trained models}
\label{modelValid}

Validation of the models was performed using a dataset containing records of 16 335 patients, accounting for over 9\% of our total patient population.
\\

In the case of the MLP model for cost prediction, we obtained a validation loss of 0.0305 and an accuracy of 80.1\%. Both loss and accuracy improved compared to the model trained on a subset of the data, which had a loss of 0.0349 and an accuracy of 78\%. However, these differences are relatively small. Additionally, the validation loss and accuracy are on par with the training metrics, indicating that the model does not suffer from overfitting.
\\

In our application, we ultimately won't use the cost categories of predicted records as they are but will aggregate them into a single category per patient. If the model occasionally predicts a category that is off by one in either direction, the final results will be affected only slightly, as there is a high likelihood that these errors might offset each other. Therefore, if we consider the percentage of cases where the model predicted either the precise category or one off, we achieve an adjusted accuracy of 96.8\% (and 99.2\% if we allow the model to be off by two categories). 
\\

Another interesting statistic to examine is the model's accuracy in each cost category to ensure it can predict correctly across all categories, not just those with the majority of the data. Here, accuracy is defined as the ratio of records successfully assigned to a category relative to all records that should belong to that category.  The results of this check can be seen in Tab. \ref{tab:cat_acc}, where we observe that accuracy declines with the frequency of categories in the dataset. However, despite the low frequency of categories 5 to 7, the model still achieves over 50\% accuracy, and only two categories (8 and 9) have accuracy below 50\%, which together form a very small part of the dataset (less than 0.4\%).
\\

\begin{table}[!h]
	\centering
	\begin{tabular}{|l|l|l|}
		\hline
		Category & Dataset frequency & Validation accuracy \\ \hline
		1        & 34.7\%           & 87.5\%              \\ \hline
		2        & 37.6\%           & 84.4\%              \\ \hline
		3        & 15.0\%           & 62.5\%              \\ \hline
		4        & 7.0\%            & 54.8\%              \\ \hline
		5        & 3.3\%            & 53.7\%              \\ \hline
		6        & 1.1\%            & 50.8\%              \\ \hline
		7        & 0.9\%            & 58.1\%              \\ \hline
		8        & 0.2\%            & 35.3\%              \\ \hline
		9        & 0.2\%            & 38.2\%              \\ \hline
	\end{tabular}
	\caption{Accuracies of record cost prediction model in each possible prediction category.}
	\label{tab:cat_acc}
\end{table}
 
For the Decoder-only Transformer model trained to predict future records, the final validation loss was 0.4740. This is significantly better than the validation loss of 0.6212 for the same model trained on the subset of data and matches or exceeds the training errors of all other model configurations tested during hyperparameter tuning.
\newpage

\section{Testing of patient future cost prediction}
\label{pat_fut_res}

The last 1,000 patients, who were not used in either training or validation, were employed to test whether the combination of trained models could succeed in our primary task: predicting the cost of a patient for the next year.
\\

Since the cost for an entire year can be significantly larger than the cost for a single record, we decided to adjust the intervals for categories accordingly. The intervals for patient costs can be seen in Tab. \ref{tab:patCost}, where we adopted an approach in which the threshold for the next category is approximately double that of the previous one.
\\

\begin{table}[!h]
	\centering
	\begin{tabular}{|l|l|}
		\hline
		Category  & Interval \\ \hline
		1 & $\interval[{0,100})$ \\ \hline
		2 & $\interval[{100,200})$ \\ \hline
		3 & $\interval[{200,500})$ \\ \hline
		4 & $\interval[{500,1000})$ \\ \hline
		5 & $\interval[{1000,2000})$ \\ \hline
		6 & $\interval[{2000,5000})$ \\ \hline
		7 & $\interval[{5000,10000})$ \\ \hline
		8 & $\interval[{10000,20000})$ \\ \hline
		9 & $\interval[{20000,\infty})$ \\ \hline
	\end{tabular}
	\caption{Intervals of patient cost for each category.}
	\label{tab:patCost}
\end{table}  

Under normal circumstances, the workflow for computing a patient’s future cost would look like this:

\begin{enumerate}
	\item Load patient data.
	\item Embed patient records.
	\item Compute the number of future records.
	\item Predict the given number of future records.
	\item Predict the cost of future records.
	\item Transform the sum of future record costs into the patient’s future cost category.
\end{enumerate}

However, since we aim to test our model, we modify this workflow slightly. After loading patient data, records from the last year are separated from the rest and are not seen by the model. Using the remaining records, the model predicts costs based on the predefined workflow. Afterwards, the cost is extracted from the separated records, and the real cost category of the patient for that year is compared to the model’s predictions to assess accuracy or proximity.