% !TeX spellcheck = en_EN-English

\section{Prediction of future records}
\label{recordPredImple}

For the task of predicting future patient records based on their history, we decided to use two models: LSTM and Transformer. We briefly experimented with simpler models, such as basic RNN and GRU, but in both cases, we immediately observed significantly worse results. Therefore, we chose to only briefly show them in our results entirely and focus on the more complex models.
\\

In both tested models, we optimized the model depth, which refers to the number of hidden layers, and the dropout rate, which is the percentage of neurons randomly deactivated (set to zero) at each training step. This technique allows other neurons to learn patterns from those that are dropped out, helping to reduce overfitting and improve generalization. For the LSTM model, we also optimized the size of the hidden layer, and for the decoder-only Transformer, we optimized the number of transformer heads.
\\  

Since our embedding of a patient record consists of multiple parts of varying lengths, we modified the loss function to account for these differences, ensuring each part contributes equally to the total loss. We based this custom loss function on the mean squared error (MSE), defined as:

\begin{equation}
	\label{eqn:mse}
	MSE = \frac{1}{n}\sum_{i=1}^{n}(Y_i-\hat{Y_i})^2.
\end{equation} 

Here, $n$ is the number of dimensions, $Y_i$ is the target value at the $i$-th dimension, and $\hat{Y_i}$ is the value predicted by the model at that dimension. However, this approach assigns more importance to parts with more dimensions. For example, if a vector has 30 dimensions divided into two parts (20 and 10 dimensions), the larger part would naturally receive twice the importance due to its size. To address this, we computed the MSE loss for each part and then calculated the mean of these losses to obtain the final loss. We refer to this modified loss function as the subpart weighted MSE, as it adjusts the weights of each dimension to give equal importance to each subpart of the vector. The formula for this loss is:

\begin{equation}
	\label{eqn:swmse}
	SubpartWeightedMSE = \frac{1}{p} \sum_{j=1}^{p}(\frac{1}{l_j}\sum_{i=1}^{l_j}(Y_{s_j+i}-\hat{Y}_{s_j+i})^2).
\end{equation} 

In this equation, we have a few additional attributes: $p$ represents the number of subparts, $l_j$ is the number of dimensions of the $j$-th subpart, and $s_j$ is the index of the first dimension of the $j$-th subpart. For the optimizer, we used the same one as in the cost of record prediction model, which is Adam.
\\

When we began trying to train these models, we encountered memory limitations, so we decided to limit the context window, meaning the number of past records seen by the model. The whole process of data preparation for training consists of this sequence of steps:

\begin{enumerate}
	\item Get batch of patients.
	\item For each patient in batch:
	\begin{enumerate}
		\item Retrieve all records for the patient.
		\item Order the patient's records.
		\item Using a moving window of the maximal context size, which moves by one record at a time, generate a list of all windows for the patient.
		\item Generate the input list by removing the last window and the target list by removing the first window.
	\end{enumerate}
	\item Collect inputs and targets for all patients in the batch.
\end{enumerate}

In step 2(b), we primarily order by date in ascending order, and if we have multiple records with the same date, we secondarily order them based on the internal ID value of diagnosis, medical procedure, and drug, in this order. We do this to make sure the ordering is coherent throughout all training inputs.
\\ 

By generating the input and target lists as explained in step 2(c), we use the $i$-th window as input and expect the $(i+1)$-st as output. We did this to increase the amount of information the model can learn by asking it to predict from incomplete context. This means that if we have a context of size $n$, we do not only want to predict the $(n+1)$-st record but also, for all $k$ such that $0<k<n$, we want to predict the $(k+1)$-st record based on the first $k$ records from the context. Both of our tested models support this behavior. In the case of the LSTM model, it is integrated directly in the model backbone provided by the \texttt{Pytorch} library since we are not using the bidirectional version of the model \cite{pytorchLSTM}, and to achieve this in the Decoder-only transformer model, we used causal masking inside the self-attention layers, which forbids queries from seeing keys from the future based on their perspective.
\\

Similarly to cost prediction models, these models were also first trained on a smaller subset of data to assess the best values for the hyperparameters being optimized, and the best one was then retrained on the complete dataset on the server. Another similarity is that we used batch training for these models as well. However, in this case, since transforming patient records into lists of windows causes them to increase in memory size, we use two types of batches: the first is the patient batch, which is a group of patients whose records are transformed together, and then the training batch, which is the usual type of batch, meaning the number of windows that go into the model at once.