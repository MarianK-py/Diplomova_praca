% !TeX spellcheck = en_EN-English

\section{Prediction of record cost}
\label{costPredRes}

In the case of predicting the record cost category, we planned to use an MLP model. All models we tested have in common that, as input, they take a 196-dimensional vector, which is the dimensionality of our embedding. After that, there are multiple hidden linear layers with non-linear functions between them, with the final layer outputting a 9-dimensional vector, which corresponds to the number of cost categories, followed by a softmax function to transform this vector into probabilities for each category. For the purpose of loss computation, we compared the correct category to the raw result from the model, while for accuracy computation, we first transformed this result into a one-hot vector based on the maximal value in the resulting vector before comparing it to the correct category.
\\

Each model that was trained to assess the best parameters was trained on a subset of the dataset containing 800 patients for training and 200 for validation. In terms of the number of records, the training dataset consisted of 2 132 436 records, while the validation set contained 533 110 records.
\\

As mentioned in Sec. \ref{costPredImple}, we primarily focused on assessing the optimal depth, layer sizes, and non-linear activation functions between those layers, but we also tested multiple different loss functions. Given the large number of possible depths and combinations of sizes and activation functions, we approached this systematically. For the number of layers (depth), we tested models with depths denoted as 0, 1, 3, and 6. A depth of 0 means the model contains a single layer to transform the input vector into a category vector, followed by a softmax function; this served as a baseline to see if the model could learn anything meaningful from the data. Depths of 1, 3, and 7 indicate that there are 1, 3, or 7 additional layers (respectively) between the input vector and the output layer, each with their own activation functions. We chose these values to represent shallow, moderately deep, and relatively deep models.
\\

For each non-zero additional depth, we tried multiple configurations to determine if we would obtain significantly different results. We tested three configuration types from the perspective of layer sizes:

\begin{itemize}
	\item Gradually decreasing layer size from input to output.
	\item Initially increasing the size from input to allow the model to learn more patterns, then decreasing it to the output size.
	\item Maintaining the input size until the very last layer, which decreases to the output size.
\end{itemize}

For each of these layer size configurations, we tested two non-linear activation function configurations:

\begin{itemize}
	\item Using only Gaussian Error Linear Units (GELU) between each layer
	\item A random combination of different activation functions selected from a predefined list
\end{itemize}

We selected activation functions for models with random combinations from the following list:

\begin{itemize}
	\item Sigmoid function (Sigmoid)
	\item Hyperbolic tangent function (Tanh)
	\item Rectified linear unit function (ReLU)
	\item Leaky Rectified Linear Unit function (LeakyReLU)
	\item Gaussian Error Linear Units function (GELU)
	\item Sigmoid Linear Unit function (SiLU)
\end{itemize}

We selected these functions to include a mix of more traditional options, such as the Sigmoid function, and more modern variants, like the Sigmoid Linear Unit function (SiLU). This approach resulted in a total of 19 different models, derived from multiplying three layer size configurations by two activation function configurations by three non-zero additional depths, plus a base model with zero additional depth. We conducted this testing three times using three different loss functions, yielding a total of 67 models. The resulting accuracies are compiled in Tab. \ref{tab:MSE_MLP}.
\\


The first loss function we tested was the mean square error (MSE) loss, which, as the name suggests, computes the average of the squared differences between the model output and the target. When examining the results for this loss function in Tab. \ref{tab:MSE_MLP}, we observe that adding layers improves model accuracy but only up to a certain point. A model with only a single layer followed by a softmax function achieved the lowest accuracy, barely exceeding 60\%, while nearly all other models surpassed 70\% accuracy. However, as we deepen the model further, we reach diminishing returns, with almost no improvement once the model has three or more layers, and accuracy slowly approaches 80\%. We also observe no signs of overfitting, as the differences between training and validation accuracy were minimal in all cases. From the perspective of layer size architecture, the best results were generally achieved by models that first expanded the dimensionality before reducing it to the number of categories.
\\

The second loss function we tested was cross-entropy loss, which is theoretically better suited for classification tasks like ours. The results show an interesting pattern: very deep models performed similarly to or even worse than shallower ones, while models with intermediate depths achieved the best performance. This could potentially stem from issues like vanishing gradients. From the perspective of layer sizes and activation functions, models using this loss exhibited behavior similar to those trained with MSE loss. Overall, the accuracy of these models was almost always lower compared to models using MSE loss.
\\

The final loss function we evaluated was negative log-likelihood (NLL) loss, which, like cross-entropy loss, is designed for classification problems. The results closely mirrored those of MSE loss across all metrics, with only minor differences: slightly lower accuracy in shallower models. However, for the deepest models tested, we observed minimal to no measurable difference in performance.
\\  

\renewcommand{\arraystretch}{1.8}
\begin{table}[!h]
	\centering
	\tiny
	%\scriptsize
	%\footnotesize
	%\small
	\begin{tabular}{|l|l|l|ll|ll|ll|}
		\hline
		\multirow{2}{*}{Depth} & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Layer \\ sizes\end{tabular}}                           & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Activation\\ functions\end{tabular}}                     & \multicolumn{2}{l|}{Mean square error}                                                                                                         & \multicolumn{2}{l|}{Cross entropy}                                                                                                             & \multicolumn{2}{l|}{Negative log likelihood}                                                                                                   \\ \cline{4-9} 
		&                                                                                                   &                                                                                                     & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Test \\ accuracy\end{tabular}} & \begin{tabular}[c]{@{}l@{}}Validation \\ accuracy\end{tabular} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Test \\ accuracy\end{tabular}} & \begin{tabular}[c]{@{}l@{}}Validation \\ accuracy\end{tabular} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Test \\ accuracy\end{tabular}} & \begin{tabular}[c]{@{}l@{}}Validation \\ accuracy\end{tabular} \\ \hline
		0                      & []                                                                                                & []                                                                                                  & \multicolumn{1}{l|}{63.2\%}                                                   & 63.0\%                                                         & \multicolumn{1}{l|}{63.5\%}                                                   & 63.3\%                                                         & \multicolumn{1}{l|}{62.9\%}                                                   & 62.6\%                                                         \\ \hline
		\multirow{6}{*}{1}     & \multirow{2}{*}{[98]}                                                                             & [GELU]                                                                                              & \multicolumn{1}{l|}{74.5\%}                                                   & 74.3\%                                                         & \multicolumn{1}{l|}{72.8\%}                                                   & 72.6\%                                                         & \multicolumn{1}{l|}{73.2\%}                                                   & 73.0\%                                                         \\ \cline{3-9} 
		&                                                                                                   & [Tanh]                                                                                              & \multicolumn{1}{l|}{71.3\%}                                                   & 71.0\%                                                         & \multicolumn{1}{l|}{70.9\%}                                                   & 70.6\%                                                         & \multicolumn{1}{l|}{71.1\%}                                                   & 70.9\%                                                         \\ \cline{2-9} 
		& \multirow{2}{*}{[392]}                                                                            & [GELU]                                                                                              & \multicolumn{1}{l|}{76.6\%}                                                   & 76.3\%                                                         & \multicolumn{1}{l|}{74.2\%}                                                   & 74.0\%                                                         & \multicolumn{1}{l|}{75.5\%}                                                   & 75.2\%                                                         \\ \cline{3-9} 
		&                                                                                                   & [Sigmoid]                                                                                           & \multicolumn{1}{l|}{70.8\%}                                                   & 70.6\%                                                         & \multicolumn{1}{l|}{69.9\%}                                                   & 69.6\%                                                         & \multicolumn{1}{l|}{70.2\%}                                                   & 70.0\%                                                         \\ \cline{2-9} 
		& \multirow{2}{*}{[196]}                                                                            & [GELU]                                                                                              & \multicolumn{1}{l|}{75.9\%}                                                   & 75.6\%                                                         & \multicolumn{1}{l|}{74.1\%}                                                   & 73.8\%                                                         & \multicolumn{1}{l|}{74.1\%}                                                   & 73.8\%                                                         \\ \cline{3-9} 
		&                                                                                                   & [LeakyReLU]                                                                                         & \multicolumn{1}{l|}{76.0\%}                                                   & 75.8\%                                                         & \multicolumn{1}{l|}{75.5\%}                                                   & 75.3\%                                                         & \multicolumn{1}{l|}{74.1\%}                                                   & 74.0\%                                                         \\ \hline
		\multirow{6}{*}{3}     & \multirow{2}{*}{[98, 48, 24]}                                                                     & [GELU, GELU, GELU]                                                                                  & \multicolumn{1}{l|}{74.4\%}                                                   & 74.2\%                                                         & \multicolumn{1}{l|}{72.0\%}                                                   & 71.8\%                                                         & \multicolumn{1}{l|}{72.2\%}                                                   & 72.0\%                                                         \\ \cline{3-9} 
		&                                                                                                   & [Sigmoid, ReLU, Tanh]                                                                               & \multicolumn{1}{l|}{69.7\%}                                                   & 69.5\%                                                         & \multicolumn{1}{l|}{69.7\%}                                                   & 69.4\%                                                         & \multicolumn{1}{l|}{70.3\%}                                                   & 70.1\%                                                         \\ \cline{2-9} 
		& \multirow{2}{*}{[392, 196, 98]}                                                                   & [GELU, GELU, GELU]                                                                                  & \multicolumn{1}{l|}{77.7\%}                                                   & 77.5\%                                                         & \multicolumn{1}{l|}{75.2\%}                                                   & 75.0\%                                                         & \multicolumn{1}{l|}{75.5\%}                                                   & 75.3\%                                                         \\ \cline{3-9} 
		&                                                                                                   & [SiLU, ReLU, GELU]                                                                                  & \multicolumn{1}{l|}{77.4\%}                                                   & 77.2\%                                                         & \multicolumn{1}{l|}{74.8\%}                                                   & 74.5\%                                                         & \multicolumn{1}{l|}{75.2\%}                                                   & 75.0\%                                                         \\ \cline{2-9} 
		& \multirow{2}{*}{[196, 196, 196]}                                                                  & [GELU, GELU, GELU]                                                                                  & \multicolumn{1}{l|}{77.2\%}                                                   & 77.0\%                                                         & \multicolumn{1}{l|}{74.7\%}                                                   & 74.3\%                                                         & \multicolumn{1}{l|}{75.0\%}                                                   & 74.7\%                                                         \\ \cline{3-9} 
		&                                                                                                   & [ReLU, Sigmoid, SiLU]                                                                               & \multicolumn{1}{l|}{76.7\%}                                                   & 76.5\%                                                         & \multicolumn{1}{l|}{73.9\%}                                                   & 73.7\%                                                         & \multicolumn{1}{l|}{75.0\%}                                                   & 74.8\%                                                         \\ \hline
		\multirow{6}{*}{7}     & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}[142, 104, 72, \\ 48, 30, 18, 12]\end{tabular}}        & \begin{tabular}[c]{@{}l@{}}[GELU, GELU, GELU, \\ GELU, GELU, \\ GELU, GELU]\end{tabular}            & \multicolumn{1}{l|}{77.1\%}                                                   & 76,9\%                                                         & \multicolumn{1}{l|}{72.0\%}                                                   & 71.7\%                                                         & \multicolumn{1}{l|}{76.5\%}                                                   & 76.2\%                                                         \\ \cline{3-9} 
		&                                                                                                   & \begin{tabular}[c]{@{}l@{}}[SiLU, Sigmoid, GELU, \\ Tanh, ReLU, \\ Sigmoid, LeakyReLU]\end{tabular} & \multicolumn{1}{l|}{75.1\%}                                                   & 75.0\%                                                         & \multicolumn{1}{l|}{70.3\%}                                                   & 70.1\%                                                         & \multicolumn{1}{l|}{75.1\%}                                                   & 74.9\%                                                         \\ \cline{2-9} 
		& \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}[588, 294, 147, \\ 49, 98, 36, 18]\end{tabular}}       & \begin{tabular}[c]{@{}l@{}}[GELU, GELU, GELU, \\ GELU, GELU, \\ GELU, GELU]\end{tabular}            & \multicolumn{1}{l|}{77.6\%}                                                   & 77.4\%                                                         & \multicolumn{1}{l|}{71.2\%}                                                   & 71.0\%                                                         & \multicolumn{1}{l|}{77.4\%}                                                   & 77.2\%                                                         \\ \cline{3-9} 
		&                                                                                                   & \begin{tabular}[c]{@{}l@{}}[SiLU, GELU, Sigmoid, \\ GELU, SiLU, \\ GELU, LeakyReLU]\end{tabular}    & \multicolumn{1}{l|}{78.3\%}                                                   & 78.0\%                                                         & \multicolumn{1}{l|}{72.0\%}                                                   & 71.8\%                                                         & \multicolumn{1}{l|}{76.9\%}                                                   & 76.7\%                                                         \\ \cline{2-9} 
		& \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}[196, 196, 196, \\ 196, 196, 196,\\ 196]\end{tabular}} & \begin{tabular}[c]{@{}l@{}}[GELU, GELU, GELU, \\ GELU, GELU, \\ GELU, GELU]\end{tabular}            & \multicolumn{1}{l|}{77.3\%}                                                   & 77.1\%                                                         & \multicolumn{1}{l|}{72.8\%}                                                   & 72.6\%                                                         & \multicolumn{1}{l|}{77.4\%}                                                   & 77.2\%                                                         \\ \cline{3-9} 
		&                                                                                                   & \begin{tabular}[c]{@{}l@{}}[Sigmoid, GELU, ReLU, \\ SiLU, LeakyReLU,\\ GELU, SiLU]\end{tabular}     & \multicolumn{1}{l|}{77.1\%}                                                   & 76.9\%                                                         & \multicolumn{1}{l|}{72.8\%}                                                   & 72.6\%                                                         & \multicolumn{1}{l|}{77.2\%}                                                   & 77.0\%                                                         \\ \hline
	\end{tabular}
	\caption{Accuracies for various MLP models using different loss functions.}
	\label{tab:MSE_MLP}
\end{table}

Based on these results, we decided to use the model from row 17 in Tab. \ref{tab:MSE_MLP} for training on the complete dataset. This model is an 8-layered model (7 additional layers plus a final layer with a softmax activation function) featuring an initially expanding layer size architecture, random activation functions, and MSE loss, which achieved the highest accuracy in our testing. Since the layer sizes, activation functions, and depth were chosen somewhat arbitrarily, we conducted additional testing to fine-tune these parameters further. However, we did not find a setup that yielded tangibly higher accuracy, so we decided to proceed with the current configuration.
\\

Before moving to the main training, we compared the MLP approach to two other types of classifiers. The first was a Gradient Boosting Classification Tree model, for which we trained trees with maximum depths of 1, 5, 10, and 20. The second classifier we tried was a Ridge regression model, where we tested multiple values for the parameter alpha, which denotes regularization strength. We used values of $10^{-10}, 10^{-5}, 0.1, 1, 10,$ and $10^3$.
\\

\begin{table}[!h]
	\centering
	\begin{tabular}{|l|l|l|}
		\hline
		Maximum depth & Train accuracy & Validation accuracy \\ \hline
		1             & 51.5\%         & 51.4\%              \\ \hline
		5             & 69.0\%         & 68.8\%              \\ \hline
		10            & 71.5\%         & 71.4\%              \\ \hline
		20            & 71.2\%         & 71.0\%              \\ \hline
	\end{tabular}
	\caption{Accuracies of Gradient Boosting models with different maximal depths of tree.}
	\label{tab:GB}
\end{table}

The results of the Gradient Boosting model are shown in Tab. \ref{tab:GB}. We observe an initial increase in accuracy; however, beyond a depth of 10, no further improvements occur. The best-performing model appears to be the one with a maximum depth of 10, achieving approximately 71\% accuracy. This is significantly lower than the 78\% accuracy of our top MLP model.
\\

\begin{table}[!h]
	\centering
	\begin{tabular}{|l|l|l|}
		\hline
		Alpha      & Train accuracy & Validation accuracy \\ \hline
		$10^{-10}$ & 67.1\%         & 66.9\%              \\ \hline
		$10^{-5}$  & 67.0\%         & 66.8\%              \\ \hline
		$10^{-1}$  & 67.0\%         & 66.8\%              \\ \hline
		$10^{0}$   & 67.0\%         & 66.8\%              \\ \hline
		$10^{1}$   & 66.9\%         & 66.7\%              \\ \hline
		$10^{3}$   & 66.8\%         & 66.7\%              \\ \hline
	\end{tabular}
	\caption{Accuracies of Ridge models with different regularization strength.}
	\label{tab:Ridge}
\end{table}

The results of the Ridge model testing are shown in Tab. \ref{tab:Ridge}, where we can see that different regularization strengths have no tangible effect on model performance. There appears to be a very small decrease in accuracy with increased regularization strength, but it is not significant. All models achieve an accuracy of around 67\%, which is higher than the most basic MLP or Gradient Boosting models, but is easily outperformed by their deeper versions.
\\

We can see that neither of the models used for comparison was able to outperform our MLP model, so we saw no reason to use any of these models instead. Therefore, we proceeded with our best MLP model, as assessed earlier, for the main training. This model achieved a training accuracy of 78.3\% and a validation accuracy of 78.0\%. The training loss for this model was 0.0346, and the validation loss was 0.0349.
