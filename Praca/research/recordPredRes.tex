% !TeX spellcheck = en_EN-English

\section{Prediction of future records}
\label{recordPredRes}

As stated in Sec. \ref{recordPredImple}, we tested two types of models: standard RNNs and a Decoder-only Transformer. For both architectures, we evaluated multiple hyperparameter combinations to identify the most effective configuration.
\\

We trained each model on the same subset of data, which consisted of 100 patients, with a total of 270 351 records, training each model for 5 epochs. We then validated each model on data from 20 patients, which were different from those used in training, and which had 54 429 records in total. These numbers are significantly lower than in the assessment of parameters for the model to predict the cost of a record, because the training of these models was significantly more complex in terms of both computational and memory requirements.
\\

Firstly, we needed to assess which standard RNN model to test, specifically whether a basic Elman RNN would suffice for our task or if using a more complex model like GRU or LSTM would yield tangible improvements. To do this, we tested all three models under the same conditions. The first setup we evaluated consisted of 6 layers with a width of 196 and a 20\% dropout rate for regularization. With this setup, we obtained the results shown in Tab. \ref{tab:rnn_comp_1}, where we observed that the LSTM model achieved the lowest loss on both training and validation data. The Elman RNN had the second-lowest training loss, while the GRU had the second-lowest validation loss. Based on these results, we concluded that the LSTM model provided tangible improvements.
\\ 

\begin{table}[!h]
  	\centering
  	\begin{tabular}{|l|l|l|}
  		\hline
  		Model        & Train loss      & Validation loss \\ \hline
  	    Elman RNN    &  0.5757         & 0.6689          \\ \hline      
  		GRU          &  0.6278         & 0.6517          \\ \hline           
  		LSTM         &  0.5156         & 0.6328          \\ \hline
  	\end{tabular}
  	\caption{Comparison of RNN models with 6 layers of width 196 and 20\% dropout rate.}
  	\label{tab:rnn_comp_1}
\end{table}

To ensure these results were not a fluke, we tested another setup. This time, we used a significantly larger model with 12 layers (double the depth) and a width of 784 (quadruple the original width), while retaining the 20\% dropout rate. The results from this setup are shown in Tab. \ref{tab:rnn_comp_2}. Here, we observe that while the GRU model appears marginally better based on training loss, the LSTM still retains a slight edge in validation loss over both other models. We conclude that although performance differences diminish with increased model size, the LSTM maintains a small advantage compared to simpler architectures.
\\

\begin{table}[!h]
	\centering
	\begin{tabular}{|l|l|l|}
		\hline
		Model        & Train loss      & Validation loss \\ \hline
		Elman RNN    &  0.6107         & 0.6404          \\ \hline      
		GRU          &  0.5467         & 0.6329          \\ \hline           
		LSTM         &  0.5780         & 0.6268          \\ \hline
	\end{tabular}
	\caption{Comparison of RNN models with 12 layers of width 784 and 20\% dropout rate.}
	\label{tab:rnn_comp_2}
\end{table}

Based on the results of these two tested configurations, we decided to use the LSTM model for further testing.

\subsection{LSTM}

For the LSTM, we were interested in finding the best combination of the number and size of hidden LSTM layers. Additionally, we wanted to determine whether adjusting the dropout rate would improve the model.
\\

As for depth, we chose three values for initial testing: 3, 6, and 12. These values represented a relatively shallow model, a slightly deeper one, and a comparably significantly deeper one. For the width of these layers, we chose 196 (matching the embedding size), then doubled and quadrupled that size to 392 and 784, respectively. In both cases, we were interested in whether increasing the model size in their corresponding dimensions would bring a sizable improvement in output quality. These two hyperparameters gave us nine combinations to test. In each test, we set the dropout rate to 20\%. Testing of different dropout rates was conducted afterward on the best model from this phase.
\\

\begin{table}[!h]
	\centering
	\begin{tabular}{|l|l|l|l|}
		\hline
		Number of layers    & Width of layer & Train loss & Validation loss \\ \hline
		\multirow{3}{*}{3}  & 196               &  0.4921         & 0.6389                \\ \cline{2-4} 
		& 392              &  0.4472         & 0.6517                \\ \cline{2-4} 
		& 784              & 0.4114          & 0.6535                \\ \hline
		\multirow{3}{*}{6}  & 196               & 0.5156          & 0.6326                \\ \cline{2-4} 
		& 392              & 0.4786          &   0.6452              \\ \cline{2-4} 
		& 784              & 0.4285          &  0.6493               \\ \hline
		\multirow{3}{*}{12} & 196               & 0.5983          & 0.6278                \\ \cline{2-4} 
		& 392              & 0.5822          & 0.6292                \\ \cline{2-4} 
		& 784              & 0.5780          & 0.6268                \\ \hline
	\end{tabular}
	\caption{Test and validation loss for different number of layers and width of layer in LSTM model.}
	\label{tab:lstm_train}
\end{table}

The results of these tests are shown in Tab. \ref{tab:lstm_train}. From the perspective of model width, we observe that shallower models (3 and 6 layers) exhibit a decrease in training loss as width increases. However, this is accompanied by an increase in validation loss, indicating potential overtraining in wider configurations. For deeper models (12 layers), there is a slight decrease in training loss, while validation loss remains mostly consistent across all widths.
\\

When analyzing model performance by depth, we observe similar behavior regardless of width. Counter-intuitively, increasing depth correlates with higher training loss, which may suggest training issues such as insufficient regularization, vanishing gradients, or suboptimal learning rates. Conversely, the slight decrease in validation loss implies that deeper models generalize better.
\\

For dropout rate testing, we selected the 12-layer LSTM model (which significantly outperformed shallower models in validation loss) with a layer width of 196. We chose this width as it showed marginally better results in shallow models and comparable performance to wider models in deeper architectures.
\\

\begin{table}[!h]
	\centering
	\begin{tabular}{|l|l|l|}
		\hline
		Dropout rate & Train loss & Validation loss \\ \hline
		10\%         & 0.5936    & 0.6351                \\ \hline
		15\%         & 0.5964    & 0.6362                \\ \hline
		20\%         & 0.5983    & 0.6278                \\ \hline
		25\%         & 0.5915    & 0.6342                \\ \hline
		30\%         & 0.6257    & 0.6550                \\ \hline 
	\end{tabular}
	\caption{Test and validation loss for different dropout rate in LSTM model with 12 layers of width 196.}
	\label{tab:lstm_dropout}
\end{table}

In Tab. \ref{tab:lstm_dropout}, we see that the optimal dropout rate appears to be 20\%, which resulted in a validation loss of 0.6278. We consider this model to be the best LSTM model we have.

\subsection{Decoder-only Transformer}

In the case of the Transformer model, when trying to find the most suitable model, we focused on three hyperparameters: two directly influencing the model (the number of Transformer layers and the number of heads) and one influencing training (the dropout rate).
\\

The hyperparameter setting process was very similar to that for the LSTM. First, we tried to assess the best values for the first two hyperparameters that influence model structure with the dropout rate set to 20\%. Once we obtained the best results, we tested a couple of dropout rate values on that specific model. For the number of layers, we tested the same model depths as for the LSTM: a shallow model with only 3 layers, a slightly deeper model with 6 layers, and a relatively deep model with 12 layers to evaluate whether deepening the model would have a positive effect on the results. In the case of the number of heads hyperparameter, we were constrained by the fact that this number must be a divisor of the number of dimensions in the input embedding, as the model splits this embedding equally into the heads. Since our embedding has 196 dimensions, with a prime factorization of $2^2\cdot7^2$, we chose three values to test: 7, 14, and 49. This allowed us to see if the model would benefit more from a larger head size or a higher number of heads. These two hyperparameter values gave us nine combinations to test.
\\

\begin{table}[!h]
	\centering
	\begin{tabular}{|l|l|l|l|}
		\hline
		Number of layers    & Number of heads & Train loss & Validation loss \\ \hline
		\multirow{3}{*}{3}  & 7        &  0.5057         &  0.6276               \\ \cline{2-4} 
		& 14              &  0.5035         &  0.6266               \\ \cline{2-4} 
		& 49              &  0.5103         &  0.6297               \\ \hline
		\multirow{3}{*}{6}  & 7       & 0.4737          &  0.6416               \\ \cline{2-4} 
		& 14              &  0.4714         &  0.6398               \\ \cline{2-4} 
		& 49              &  0.4716         &  0.6439               \\ \hline
		\multirow{3}{*}{12} & 7       & 0.4624          & 0.6537                \\ \cline{2-4} 
		& 14              & 0.4301          &  0.6704               \\ \cline{2-4} 
		& 49              & 0.4119          &  0.6729               \\ \hline
	\end{tabular}
	\caption{Test and validation loss for different number of layers and number of heads in Transformer model.}
	\label{tab:transformer_train}
\end{table}

If we examine the performance of the models shown in Tab. \ref{tab:transformer_train}, we observe that for shallower models (3 and 6 layers), differences in head counts yield negligible results. The best value appears to be 14 heads in both cases, though this might be coincidental due to the minimal performance gaps. For the deeper 12-layer model, increasing the number of heads reduces training loss (improving fit to training data) but increases validation loss (worsening generalization).
\\

Regarding model depth, we see behavior opposite to the LSTM results: deeper Transformer models exhibit significantly lower training loss but higher validation loss, likely due to overfitting.
\\

Given these signs of overtraining in deeper models, we selected shallow 3-layer architectures for dropout rate testing. Specifically, we chose the model with 14 heads, which achieved the best training and validation loss among all 3-layer models.
\\

\begin{table}[!h]
	\centering
	\begin{tabular}{|l|l|l|}
		\hline
		Dropout rate & Train loss & Validation loss \\ \hline
		10\%         & 0.4840    & 0.6433                \\ \hline
		15\%         & 0.4889    & 0.6372                \\ \hline
		20\%         & 0.5035    & 0.6266                \\ \hline
		25\%         & 0.5105    & 0.6232                \\ \hline
		30\%         & 0.5200    & 0.6212                \\ \hline 
		35\%         & 0.5225    & 0.6214                \\ \hline
		40\%         & 0.5172    & 0.6226                \\ \hline 
	\end{tabular}
	\caption{Test and validation loss for different dropout rates in the Transformer model with 3 layers and 14 heads.}
	\label{tab:transformer_dropout}
\end{table}

In the results of dropout rate training shown in Tab. \ref{tab:transformer_dropout}, we observe that increasing the dropout rate beyond 20\% yields a tangible improvement in validation loss, although training loss increases, but only up to a certain point, beyond which no measurable improvement is seen (specifically, beyond 30\%). Based on these results, we decided to select as the best Decoder-only Transformer model one with 3 layers, 14 heads, and a 30\% dropout rate, which achieved a validation loss of 0.6212.
\\

With this, we ended up with two models to choose from: an LSTM model with a validation loss of 0.6278 and a Decoder-only Transformer with a validation loss of 0.6212. We chose the latter because it not only had a slightly lower validation loss but also a lower training loss of 0.5200 compared to the LSTM model's training loss of 0.5983.
