% !TeX spellcheck = en_EN-English

\section{Prediction of future records}
\label{recordPredRes}

As said in \ref{recordPredImple} we tried two type of models and those are standard RNN models and Decoder-only Transformer. In both cases we tried multiple combination of few hyperparameters to get most successful model. 
\\

We trained each model on same subset of data which consist of 100 patient, which have in total (get number of records) records to train model for 5 epochs and then validate each model on data from 10 patient which are different from one used in training and have (get number of records) records in total. This number is significantly lower to assessment of parameters for model to predict cost of record this training of these models was significantly more complex from perspective of both computational and memory complexity.
\\

Firstly we have to assess which of standard RNN models we wanna test, whether for our task would basic Elman RNN be sufficient or whether we would see tangible improvement by using more complex model like GRU or LSTM. For this we tried all three models with same setup. First setup we tested was 6 layers with width 196 and 20\% dropout rate for regularization. With this setup we received results shown in Tab. \ref{tab:rnn_comp_1} where we can see that LSTM model achieved lowest loss on both training and validation data, with Elman RNN having second lowest training loss while GRU having second lowest validation loss. Based on these results we could conclude that LSTM model bring tangible improvement.
\\ 

\begin{table}[!h]
  	\centering
  	\begin{tabular}{|l|l|l|}
  		\hline
  		Model        & Train loss      & Validation loss \\ \hline
  	    Elman RNN    &  0.5757         & 0.6689          \\ \hline      
  		GRU          &  0.6278         & 0.6517          \\ \hline           
  		LSTM         &  0.5156         & 0.6328          \\ \hline
  	\end{tabular}
  	\caption{Comparison of RNN models with 6 layers of width 196 and 20\% dropout rate.}
  	\label{tab:rnn_comp_1}
\end{table}

To make sure these results are not fluke we decided to test one more setup. This time tried significantly bigger model deepened to 12 layers, so twice the depth, each with width 784, so quadruple width, we still left dropout rate to be 20\%. Results we received by using this setup can be viewed in Tab. \ref{tab:rnn_comp_2}. There we can see that based on training loss best model seems to GRU, however if we look at validation loss we can see that LSTM still have slight edge over both other models. So we can conclude that even though differences in performance became less pronounce as we increase model size LSTM model still maintain slight advantage compare to simpler models.
\\

\begin{table}[!h]
	\centering
	\begin{tabular}{|l|l|l|}
		\hline
		Model        & Train loss      & Validation loss \\ \hline
		Elman RNN    &  0.6107         & 0.6404          \\ \hline      
		GRU          &  0.5467         & 0.6329          \\ \hline           
		LSTM         &  0.5780         & 0.6268          \\ \hline
	\end{tabular}
	\caption{Comparison of RNN models with 12 layers of width 784 and 20\% dropout rate.}
	\label{tab:rnn_comp_2}
\end{table}

Based on results of these two tested configurations we decided to use LSTM model for further testing.

\subsection{LSTM}

For LSTM we were interested to find best combination of number and size of hidden LSTM layers. Additionally we wanted to know whether adjusting dropout rate would improve model.
\\

As for depth we choose three values for initial testing, those were 3, 6 and 12, to have relatively shallow model, slightly deeper one and comparably significantly deeper one. For the width of these layers we choose 196 which is size embedding and then double and quadruple that size so 392 and 784. In both cases we were interested to see if increasing size of model in their corresponding dimension would have bring sizable improvement in model output quality or not. These two hyperparameters gave us 9 combination we tested, in each test we set dropout rate to 20\%, testing of different dropout rates was then afterwards on best model from this testing. 
\\

\begin{table}[!h]
	\centering
	\begin{tabular}{|l|l|l|l|}
		\hline
		Number of layers    & Width of layer & Train loss & Validation loss \\ \hline
		\multirow{3}{*}{3}  & 196               &  0.4921         & 0.6389                \\ \cline{2-4} 
		& 392              &  0.4472         & 0.6517                \\ \cline{2-4} 
		& 784              & 0.4114          & 0.6535                \\ \hline
		\multirow{3}{*}{6}  & 196               & 0.5156          & 0.6326                \\ \cline{2-4} 
		& 392              & 0.4786          &   0.6452              \\ \cline{2-4} 
		& 784              & 0.4285          &  0.6493               \\ \hline
		\multirow{3}{*}{12} & 196               & 0.5983          & 0.6278                \\ \cline{2-4} 
		& 392              & 0.5822          & 0.6292                \\ \cline{2-4} 
		& 784              & 0.5780          & 0.6268                \\ \hline
	\end{tabular}
	\caption{Test and valuation loss for different number of layers and width of layer in LSTM model.}
	\label{tab:lstm_train}
\end{table}

Results of these tests are in Tab. \ref{tab:lstm_train} where we can see that from perspective of width of a model, we can see that shallower model with 3 and 6 layers show decrease in training with as width increase, however, at the same time we increase in validation loss indicating potential overtraining of wider models. In case of deep models with 12 layers, we see slight decrease in training loss, however this time validation loss stays mostly same for all widths of a model. 
\\

When we look at performance of models from perspective of different depths we can see similar behavior indifferently of width of model. We can see by deepening model training loss increase which is counter-intuitive and might indicate issues during training like insufficient regularization, vanishing gradient or wrong learning rate. On the other hand we can slight decrease in validation loss indicating that deeper more were able to generalize information.
\\

For testing of different dropout rate we choose deep model with 12 LSTM layers as these models outperform shallower model significantly based on validation loss, and with each layer having width of 196 as these models showed slightly better results in shallower model while having comparable results to wider model in deep model.
\\

\begin{table}[!h]
	\centering
	\begin{tabular}{|l|l|l|}
		\hline
		Dropout rate & Train loss & Validation loss \\ \hline
		10\%         & 0.5936    & 0.6351                \\ \hline
		15\%         & 0.5964    & 0.6362                \\ \hline
		20\%         & 0.5983    & 0.6278                \\ \hline
		25\%         & 0.5915    & 0.6342                \\ \hline
		30\%         & 0.6257    & 0.6550                \\ \hline 
	\end{tabular}
	\caption{Test and valuation loss for different dropout rate in LSTM model with 12 layers of width 196.}
	\label{tab:lstm_dropout}
\end{table}

In Tab. \ref{tab:lstm_dropout} we see that optimal dropout rate seems to be 20\% which resulted in validation loss of 0.6278. This model we consider as the best LSTM model we have.

\subsection{Decoder-only Transformer}

In case of Transformer model when we were trying to find most suitable model we focused on three hyperparameters, two directly influencing model, those were number of transformer layers and number of heads, and one influencing training, which was dropout rate.
\\

Settling of hyperparameters was very similar to LSTM, so firstly we tried to assess best values for first two hyperparameters that influence model structure with dropout rate set to 20\% and once we got best results than we test couple of dropout rate values on that specific model. For number of layers we tested same depths of model as in LSTM meaning shallow model with only 3 layers, bit deeper model with 6 layers and relatively deep model with 12 layers to assess whether deepening model have positive effect on results. In case of number of head hyperparameter we are constricting by the fact that this number have to be divisor of number of dimension on input embedding since model split this embedding equally into the heads. Since our embedding has 196 dimension it's prime factorization is $2^2\cdot7^2$. Based on this we choose three values to test, those values were 7, 14 and 49 to see if model would profit more from bigger size of a head  or from higher number of heads. These two hyperparameters values gave us 9 combination to test.

\begin{table}[!h]
	\centering
	\begin{tabular}{|l|l|l|l|}
		\hline
		Number of layers    & Number of heads & Train loss & Validation loss \\ \hline
		\multirow{3}{*}{3}  & 7        &  0.5057         &  0.6276               \\ \cline{2-4} 
		& 14              &  0.5035         &  0.6266               \\ \cline{2-4} 
		& 49              &  0.5103         &  0.6297               \\ \hline
		\multirow{3}{*}{6}  & 7       & 0.4737          &  0.6416               \\ \cline{2-4} 
		& 14              &  0.4714         &  0.6398               \\ \cline{2-4} 
		& 49              &  0.4716         &  0.6439               \\ \hline
		\multirow{3}{*}{12} & 7       & 0.4624          & 0.6537                \\ \cline{2-4} 
		& 14              & 0.4301          &  0.6704               \\ \cline{2-4} 
		& 49              & 0.4119          &  0.6729               \\ \hline
	\end{tabular}
	\caption{Test and valuation loss for different number of layers and number of heads in Transformer model.}
	\label{tab:transformer_train}
\end{table}

If we look at performance of models shown in Tab. \ref{tab:transformer_train} we see that from perspective of head counts less deep models with 3 and 6 layers have negligible differences with the best value being 14 head in both cases which might be just coincidence since differences are so small. In case of deeper model with 12 layers we see that increasing number of heads cause decrease in train loss so model fit training data better but increase in validation loss so model is worse in generalization of data.
\\

From perspective of depth of model we can interesting see completely opposite behavior to what we saw in LSTM model, where in this case with increased depth of model training loss decreases significantly while validation loss increases, this can be most likely caused by overtraining in deep models.
\\

Since deeper models showed signs of overtraining we decided that for testing of dropout rate we would chose from shallow models with 3 layers, and more specifically we chose model with best both training and validation loss which have 14 heads. 
\\

\begin{table}[!h]
	\centering
	\begin{tabular}{|l|l|l|}
		\hline
		Dropout rate & Train loss & Validation loss \\ \hline
		10\%         & 0.4840    & 0.6433                \\ \hline
		15\%         & 0.4889    & 0.6372                \\ \hline
		20\%         & 0.5035    & 0.6266                \\ \hline
		25\%         & 0.5105    & 0.6232                \\ \hline
		30\%         & 0.5200    & 0.6212                \\ \hline 
		35\%         & 0.5225    & 0.6214                \\ \hline
		40\%         & 0.5172    & 0.6226                \\ \hline 
	\end{tabular}
	\caption{Test and valuation loss for different dropout rate in Transformer model with 3 layers and 14 heads.}
	\label{tab:transformer_dropout}
\end{table}

In results of dropout rate training shown in Tab. \ref{tab:transformer_dropout} we can see increasing dropout rate beyond 20\% has tangible improvement validation loss while training loss increases but again only to certain point where we can see no measurable improvement beyond 30\%. Based on these results we decided to choose as the best Decoder-only Transformer model one with 3 layers, 14 heads and 30\% dropout rate, which boast validation loss of 0.6212.
\\

With this we ended with two model to chose from, LSTM model with validation loss 0.6278 and Decoder-only Transformer with validation loss 0.6212. Out of these two we have chosen later one because it not only had slightly lower validation loss but also lower training loss of 0.5200 compared to training loss 0.5983 of LSTM model. 
