@preamble{ "\newcommand{\noopsort}[1]{} "
	# "\newcommand{\printfirst}[2]{#1} "
	# "\newcommand{\singleletter}[1]{#1} "
	# "\newcommand{\switchargs}[2]{#2#1} " }

% CPT code clustering 1
@inproceedings{lorenzi2017predictive,
	title={Predictive Hierarchical Clustering: Learning clusters of CPT codes for improving surgical outcomes},
	author={Lorenzi, Elizabeth C and Brown, Stephanie L and Sun, Zhifei and Heller, Katherine},
	booktitle={Machine Learning for Healthcare Conference},
	pages={231--242},
	year={2017},
	organization={PMLR}
}


% CPT code clustering 2
IGNORE FOR NOW
article{levy2022comparison,
	title={Comparison of machine-learning algorithms for the prediction of current procedural terminology (CPT) codes from pathology reports},
	author={Levy, Joshua and Vattikonda, Nishitha and Haudenschild, Christian and Christensen, Brock and Vaickus, Louis},
	journal={Journal of Pathology Informatics},
	volume={13},
	pages={100165},
	year={2022},
	publisher={Elsevier}
}

% Patient future prediction
@inproceedings{miotto2016deep,
	title={Deep learning to predict patient future diseases from the electronic health records},
	author={Miotto, Riccardo and Li, Li and Dudley, Joel T},
	booktitle={Advances in Information Retrieval: 38th European Conference on IR Research, ECIR 2016, Padua, Italy, March 20--23, 2016. Proceedings 38},
	pages={768--774},
	year={2016},
	organization={Springer}
}

% Predicting expenditures
@article{caballer2019predicting,
	title={Predicting healthcare expenditure by multimorbidity groups},
	author={Caballer-Tarazona, Vicent and Guadalajara-Olmeda, Natividad and Vivas-Consuelo, David},
	journal={Health Policy},
	volume={123},
	number={4},
	pages={427--434},
	year={2019},
	publisher={Elsevier}
}

% Predicting high-cost patients
@article{chechulin2014predicting,
	title={Predicting patients with high risk of becoming high-cost healthcare users in Ontario (Canada)},
	author={Chechulin, Yuriy and Nazerian, Amir and Rais, Saad and Malikov, Kamil},
	journal={Healthcare Policy},
	volume={9},
	number={3},
	pages={68},
	year={2014},
	publisher={Longwoods Publishing}
}

% cost prediction
@article{morid2019healthcare,
	title={Healthcare cost prediction: Leveraging fine-grain temporal patterns},
	author={Morid, Mohammad Amin and Sheng, Olivia R Liu and Kawamoto, Kensaku and Ault, Travis and Dorius, Josette and Abdelrahman, Samir},
	journal={Journal of biomedical informatics},
	volume={91},
	pages={103113},
	year={2019},
	publisher={Elsevier}
}

@inproceedings{morid2018supervised,
	title={Supervised learning methods for predicting healthcare costs: systematic literature review and empirical evaluation},
	author={Morid, Mohammad Amin and Kawamoto, Kensaku and Ault, Travis and Dorius, Josette and Abdelrahman, Samir},
	booktitle={AMIA annual symposium proceedings},
	volume={2017},
	pages={1312},
	year={2018}
}


% Embedding
@article{choi2018mime,
	title={Mime: Multilevel medical embedding of electronic health records for predictive healthcare},
	author={Choi, Edward and Xiao, Cao and Stewart, Walter and Sun, Jimeng},
	journal={Advances in neural information processing systems},
	volume={31},
	year={2018}
}

% DeepPatient -> disease prediction
@article{miotto2016deep,
	title={Deep patient: an unsupervised representation to predict the future of patients from the electronic health records},
	author={Miotto, Riccardo and Li, Li and Kidd, Brian A and Dudley, Joel T},
	journal={Scientific reports},
	volume={6},
	number={1},
	pages={1--10},
	year={2016},
	publisher={Nature Publishing Group}
}

% ICD-10-CM
@misc{cdcICD10CM,
	author = {CDC},
	title = {{I}{C}{D}-10-{C}{M} --- cdc.gov},
	howpublished = {\url{https://www.cdc.gov/nchs/icd/icd-10-cm/index.html}},
	year = {},
	note = {[Accessed 16-09-2024]},
}

% MKCH-10
@misc{ncziMKCH,
	author = {},
	title = {Medzinárodná klasifikácia chorôb - {M}{K}{C}{H}-10  --- nczisk.sk},
	howpublished = {\url{https://www.nczisk.sk/Standardy-v-zdravotnictve/Pages/Medzinarodna-klasifikacia-chorob-MKCH-10.aspx}},
	year = {},
	note = {[Accessed 16-09-2024]},
}

% ATC
@misc{atc_who,
	author = {},
	title = {{A}natomical {T}herapeutic {C}hemical ({A}{T}{C}) {C}lassification --- who.int},
	howpublished = {\url{https://www.who.int/tools/atc-ddd-toolkit/atc-classification}},
	year = {},
	note = {[Accessed 25-09-2024]},
}

%LaBSE paper
@inproceedings{labse_paper,
	title={Language-agnostic BERT Sentence Embedding},
	author={Feng, Fangxiaoyu and Yang, Yinfei and Cer, Daniel and Arivazhagan, Naveen and Wang, Wei},
	booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	pages={878--891},
	year={2022}
}

%LaBSE kaggle
@misc{labse_kaggle,
	author = {},
	title = {{G}oogle | {L}a{B}{S}{E} | {K}aggle --- kaggle.com},
	howpublished = {\url{https://www.kaggle.com/models/google/labse/tensorFlow2/labse/1?tfhub-redirect=true}},
	year = {},
	note = {[Accessed 18-10-2024]},
}

%LaBSE hugging face
@misc{labse_hug,
	author = {},
	title = {sentence-transformers/{L}a{B}{S}{E} · {H}ugging {F}ace --- huggingface.co},
	howpublished = {\url{https://huggingface.co/sentence-transformers/LaBSE}},
	year = {},
	note = {[Accessed 19-10-2024]},
}

%Word2Vec
@misc{word2vec,
	author = {},
	title = {{G}it{H}ub - essential-data/word2vec-sk: {V}ector representations of {S}lovak words trained using word2vec --- github.com},
	howpublished = {\url{https://github.com/essential-data/word2vec-sk}},
	year = {},
	note = {[Accessed 19-10-2024]},
}

% BERT
@inproceedings{bert,
	title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	pages={4171--4186},
	year={2019}
}

%BERT training 1
@inproceedings{bert_pretr_1,
	title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
	author = "Devlin, Jacob  and
	Chang, Ming-Wei  and
	Lee, Kenton  and
	Toutanova, Kristina",
	editor = "Burstein, Jill  and
	Doran, Christy  and
	Solorio, Thamar",
	booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
	month = jun,
	year = "2019",
	address = "Minneapolis, Minnesota",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/N19-1423",
	doi = "10.18653/v1/N19-1423",
	pages = "4171--4186",
	abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

%BERT training 2
@article{bert_pretr_2,
	title={NSP-Bert: A prompt-based few-shot learner through an original pre-training task--next sentence prediction},
	author={Sun, Yi and Zheng, Yu and Hao, Chao and Qiu, Hangping},
	journal={arXiv preprint arXiv:2109.03564},
	year={2021}
}

%BERT training 3
@article{bert_pretr_3,
	title={Cross-lingual language model pretraining},
	author={Conneau, Alexis and Lample, Guillaume},
	journal={Advances in neural information processing systems},
	volume={32},
	year={2019}
}

%Estimating number of visits
@article{num_of_vis,
	title={Estimating the number of visits to the doctor},
	author={Berzel, Andreas and Heller, Gillian Z and Zucchini, Walter},
	journal={Australian \& New Zealand Journal of Statistics},
	volume={48},
	number={2},
	pages={213--224},
	year={2006},
	publisher={Wiley Online Library}
}

@inproceedings{elman_img,
	title={Contextual Representation using Recurrent Neural Network Hidden State for Statistical Parametric Speech Synthesis.},
	author={Achanta, Sivanand and Banoth, Rambabu and Pandey, Ayushi and Vadapalli, Anandaswarup and Gangashetty, Suryakanth V},
	booktitle={SSW},
	pages={172--177},
	year={2016}
}

@article{elman,
	title={Finding structure in time},
	author={Elman, Jeffrey L},
	journal={Cognitive science},
	volume={14},
	number={2},
	pages={179--211},
	year={1990},
	publisher={Wiley Online Library}
}

@article{attentionAllYouNeed,
	title={Attention is all you need},
	author={Vaswani, A},
	journal={Advances in Neural Information Processing Systems},
	year={2017}
}

@article{MLParch,
	title={Hyperparameter tuning of artificial neural networks for well production estimation considering the uncertainty in initialized parameters},
	author={Jin, Miao and Liao, Qinzhuo and Patil, Shirish and Abdulraheem, Abdulazeez and Al-Shehri, Dhafer and Glatz, Guenther},
	journal={ACS omega},
	volume={7},
	number={28},
	pages={24145--24156},
	year={2022},
	publisher={ACS Publications}
}

@article{adam,
	title={Adam: A method for stochastic optimization},
	author={Kingma, Diederik P and Ba, Jimmy},
	journal={arXiv preprint arXiv:1412.6980},
	year={2014}
}

@article{chatGPT,
	title={In: ChatGPT vezia 4},
	author={Online},
	journal={Available at: OpenAI, URL},
	volume={Task: },
	year={}
	note = {[Accessed  __-__-2025]},
}

@misc{w2v,
	author = {},
	title = {{G}it{H}ub - essential-data/word2vec-sk: {V}ector representations of {S}lovak words trained using word2vec --- github.com},
	howpublished = {\url{https://github.com/essential-data/word2vec-sk}},
	year = {},
	note = {[Accessed 04-03-2025]},
}

@misc{w2vOrigPaper,
	title={Efficient Estimation of Word Representations in Vector Space}, 
	author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
	year={2013},
	eprint={1301.3781},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/1301.3781}, 
}

@phdthesis{cbow,
	author = {Lopez, Waldemar},
	year = {2019},
	month = {11},
	pages = {},
	title = {VECTOR REPRESENTATION OF INTERNET DOMAIN NAMES USING WORD EMBEDDING TECHNIQUES}
}

@article{skipgram,
	author = {Mimouni, Nada and Moissinac, Jean-Claude and Vu, Anh Tuan},
	year = {2020},
	month = {06},
	pages = {},
	title = {Domain Specific Knowledge Graph Embedding for Analogical Link Discovery}
}

@software{pandas,
	author       = {The pandas development team},
	title        = {pandas-dev/pandas: Pandas},
	month        = feb,
	year         = 2020,
	publisher    = {Zenodo},
	version      = {2.2.1},
	doi          = {10.5281/zenodo.3509134},
	url          = {https://doi.org/10.5281/zenodo.3509134}
}

@Article{         numpy,
	title         = {Array programming with {NumPy}},
	author        = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
	van der Walt and Ralf Gommers and Pauli Virtanen and David
	Cournapeau and Eric Wieser and Julian Taylor and Sebastian
	Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
	and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
	Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
	R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
	G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
	Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
	Travis E. Oliphant},
	year          = {2020},
	month         = sep,
	journal       = {Nature},
	volume        = {585},
	number        = {7825},
	pages         = {357--362},
	doi           = {10.1038/s41586-020-2649-2},
	publisher     = {Springer Science and Business Media {LLC}},
	url           = {https://doi.org/10.1038/s41586-020-2649-2}
}

@article{scikitlearn,
	title={Scikit-learn: Machine Learning in {P}ython},
	author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
	and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
	and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
	Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	journal={Journal of Machine Learning Research},
	volume={12},
	pages={2825--2830},
	year={2011}
}

@book{ntlk,
	title={Natural language processing with Python: analyzing text with the natural language toolkit},
	author={Bird, Steven and Klein, Ewan and Loper, Edward},
	year={2009},
	publisher={" O'Reilly Media, Inc."}
}

@article{gensim,
	title={Gensim--python framework for vector space modelling},
	author={Rehurek, Radim and Sojka, Petr},
	journal={NLP Centre, Faculty of Informatics, Masaryk University, Brno, Czech Republic},
	volume={3},
	number={2},
	year={2011}
}

@inproceedings{sentence_transformer,
	title = {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
	author = {Reimers, Nils and Gurevych, Iryna},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing},
	month = {11},
	year = {2019},
	publisher = {Association for Computational Linguistics},
	url = {http://arxiv.org/abs/1908.10084},
}

@software{simplemma,
	author       = {Barbaresi, Adrien},
	title        = {Simplemma},
	month        = jan,
	year         = 2023,
	publisher    = {Zenodo},
	version      = {v0.9.1},
	doi          = {10.5281/zenodo.7555188},
	url          = {https://doi.org/10.5281/zenodo.7555188},
}

@misc{pytorch,
	title={PyTorch: An Imperative Style, High-Performance Deep Learning Library}, 
	author={Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas Köpf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
	year={2019},
	eprint={1912.01703},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/1912.01703}, 
}

@misc{pytorchLSTM,
	author = {},
	title = {{L}{S}{T}{M} --- {P}y{T}orch 2.6 documentation --- pytorch.org},
	howpublished = {\url{https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html}},
	year = {},
	note = {[Accessed 23-03-2025]},
}
