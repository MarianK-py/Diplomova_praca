@preamble{ "\newcommand{\noopsort}[1]{} "
	# "\newcommand{\printfirst}[2]{#1} "
	# "\newcommand{\singleletter}[1]{#1} "
	# "\newcommand{\switchargs}[2]{#2#1} " }

% CPT code clustering 1
@inproceedings{lorenzi2017predictive,
	title={Predictive Hierarchical Clustering: Learning clusters of CPT codes for improving surgical outcomes},
	author={Lorenzi, Elizabeth C and Brown, Stephanie L and Sun, Zhifei and Heller, Katherine},
	booktitle={Machine Learning for Healthcare Conference},
	pages={231--242},
	year={2017},
	organization={PMLR}
}


% CPT code clustering 2
IGNORE FOR NOW
article{levy2022comparison,
	title={Comparison of machine-learning algorithms for the prediction of current procedural terminology (CPT) codes from pathology reports},
	author={Levy, Joshua and Vattikonda, Nishitha and Haudenschild, Christian and Christensen, Brock and Vaickus, Louis},
	journal={Journal of Pathology Informatics},
	volume={13},
	pages={100165},
	year={2022},
	publisher={Elsevier}
}

% Patient future prediction
@inproceedings{miotto2016deep,
	title={Deep learning to predict patient future diseases from the electronic health records},
	author={Miotto, Riccardo and Li, Li and Dudley, Joel T},
	booktitle={Advances in Information Retrieval: 38th European Conference on IR Research, ECIR 2016, Padua, Italy, March 20--23, 2016. Proceedings 38},
	pages={768--774},
	year={2016},
	organization={Springer}
}

% Predicting expenditures
@article{caballer2019predicting,
	title={Predicting healthcare expenditure by multimorbidity groups},
	author={Caballer-Tarazona, Vicent and Guadalajara-Olmeda, Natividad and Vivas-Consuelo, David},
	journal={Health Policy},
	volume={123},
	number={4},
	pages={427--434},
	year={2019},
	publisher={Elsevier}
}

% Predicting high-cost patients
@article{chechulin2014predicting,
	title={Predicting patients with high risk of becoming high-cost healthcare users in Ontario (Canada)},
	author={Chechulin, Yuriy and Nazerian, Amir and Rais, Saad and Malikov, Kamil},
	journal={Healthcare Policy},
	volume={9},
	number={3},
	pages={68},
	year={2014},
	publisher={Longwoods Publishing}
}

% cost prediction
@article{morid2019healthcare,
	title={Healthcare cost prediction: Leveraging fine-grain temporal patterns},
	author={Morid, Mohammad Amin and Sheng, Olivia R Liu and Kawamoto, Kensaku and Ault, Travis and Dorius, Josette and Abdelrahman, Samir},
	journal={Journal of biomedical informatics},
	volume={91},
	pages={103113},
	year={2019},
	publisher={Elsevier}
}

% Embedding
@article{choi2018mime,
	title={Mime: Multilevel medical embedding of electronic health records for predictive healthcare},
	author={Choi, Edward and Xiao, Cao and Stewart, Walter and Sun, Jimeng},
	journal={Advances in neural information processing systems},
	volume={31},
	year={2018}
}

% DeepPatient -> disease prediction
@article{miotto2016deep,
	title={Deep patient: an unsupervised representation to predict the future of patients from the electronic health records},
	author={Miotto, Riccardo and Li, Li and Kidd, Brian A and Dudley, Joel T},
	journal={Scientific reports},
	volume={6},
	number={1},
	pages={1--10},
	year={2016},
	publisher={Nature Publishing Group}
}

% ICD-10-CM
@misc{cdcICD10CM,
	author = {CDC},
	title = {{I}{C}{D}-10-{C}{M} --- cdc.gov},
	howpublished = {\url{https://www.cdc.gov/nchs/icd/icd-10-cm/index.html}},
	year = {},
	note = {[Accessed 16-09-2024]},
}

% MKCH-10
@misc{ncziMKCH,
	author = {},
	title = {Medzinárodná klasifikácia chorôb - {M}{K}{C}{H}-10  --- nczisk.sk},
	howpublished = {\url{https://www.nczisk.sk/Standardy-v-zdravotnictve/Pages/Medzinarodna-klasifikacia-chorob-MKCH-10.aspx}},
	year = {},
	note = {[Accessed 16-09-2024]},
}

% ATC
@misc{atc_who,
	author = {},
	title = {{A}natomical {T}herapeutic {C}hemical ({A}{T}{C}) {C}lassification --- who.int},
	howpublished = {\url{https://www.who.int/tools/atc-ddd-toolkit/atc-classification}},
	year = {},
	note = {[Accessed 25-09-2024]},
}

%LaBSE paper
@inproceedings{labse_paper,
	title={Language-agnostic BERT Sentence Embedding},
	author={Feng, Fangxiaoyu and Yang, Yinfei and Cer, Daniel and Arivazhagan, Naveen and Wang, Wei},
	booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	pages={878--891},
	year={2022}
}

%LaBSE kaggle
@misc{labse_kaggle,
	author = {},
	title = {{G}oogle | {L}a{B}{S}{E} | {K}aggle --- kaggle.com},
	howpublished = {\url{https://www.kaggle.com/models/google/labse/tensorFlow2/labse/1?tfhub-redirect=true}},
	year = {},
	note = {[Accessed 18-10-2024]},
}

%LaBSE hugging face
@misc{labse_hug,
	author = {},
	title = {sentence-transformers/{L}a{B}{S}{E} · {H}ugging {F}ace --- huggingface.co},
	howpublished = {\url{https://huggingface.co/sentence-transformers/LaBSE}},
	year = {},
	note = {[Accessed 19-10-2024]},
}

%Word2Vec
@misc{word2vec,
	author = {},
	title = {{G}it{H}ub - essential-data/word2vec-sk: {V}ector representations of {S}lovak words trained using word2vec --- github.com},
	howpublished = {\url{https://github.com/essential-data/word2vec-sk}},
	year = {},
	note = {[Accessed 19-10-2024]},
}

% BERT
@inproceedings{bert,
	title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	pages={4171--4186},
	year={2019}
}

%BERT training 1
@inproceedings{bert_pretr_1,
	title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
	author = "Devlin, Jacob  and
	Chang, Ming-Wei  and
	Lee, Kenton  and
	Toutanova, Kristina",
	editor = "Burstein, Jill  and
	Doran, Christy  and
	Solorio, Thamar",
	booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
	month = jun,
	year = "2019",
	address = "Minneapolis, Minnesota",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/N19-1423",
	doi = "10.18653/v1/N19-1423",
	pages = "4171--4186",
	abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

%BERT training 2
@article{bert_pretr_2,
	title={NSP-Bert: A prompt-based few-shot learner through an original pre-training task--next sentence prediction},
	author={Sun, Yi and Zheng, Yu and Hao, Chao and Qiu, Hangping},
	journal={arXiv preprint arXiv:2109.03564},
	year={2021}
}

%BERT training 3
@article{bert_pretr_3,
	title={Cross-lingual language model pretraining},
	author={Conneau, Alexis and Lample, Guillaume},
	journal={Advances in neural information processing systems},
	volume={32},
	year={2019}
}